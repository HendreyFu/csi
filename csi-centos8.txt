====VSI
issue,
vsi web can't login after init..

172.18.0.2 - - [21/Mar/2021:05:41:26 +0000] "GET /api/ui/htmlClientSdk.js HTTP/1.0" 404 555 "https://192.168.1.19/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36" "192.168.240.8"
172.18.0.2 - - [21/Mar/2021:05:41:26 +0000] "GET /polyfills-es2015.276e6d88c5582a8066d9.js HTTP/1.0" 200 68592 "https://192.168.1.19/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36" "192.168.240.8"
2021/03/21 05:41:26 [error] 22#22: *1 open() "/usr/share/nginx/html/api/ui/htmlClientSdk.js" failed (2: No such file or directory), client: 172.18.0.2, server: localhost, request: "GET /api/ui/htmlClientSdk.js HTTP/1.0", host: "192.168.1.19", referrer: "https://192.168.1.19/"
172.18.0.2 - - [21/Mar/2021:05:41:26 +0000] "GET /styles.21dd68fec88ebe09b9f5.css HTTP/1.0" 200 534996 "https://192.168.1.19/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36" "192.168.240.8"
172.18.0.2 - - [21/Mar/2021:05:41:26 +0000] "GET /runtime-es2015.6ce2d5635f6beb0d662a.js HTTP/1.0" 200 2289 "https://192.168.1.19/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36" "192.168.240.8"
172.18.0.2 - - [21/Mar/2021:05:41:26 +0000] "GET /main-es2015.eabee8b3bc1f1d6df081.js HTTP/1.0" 200 3304819 "https://192.168.1.19/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36" "192.168.240.8"
172.18.0.2 - - [21/Mar/2021:05:41:27 +0000] "GET /assets/images/vsi-icons/icon-vsi.png HTTP/1.0" 200 2217 "https://192.168.1.19/" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.90 Safari/537.36" "192.168.240.8"


====CSI 03/21/2021
Supported Platforms and Support Kubernetes version!!!
https://dell.github.io/storage-plugin-docs/docs/dell-csi-driver/

Release Notes - CSI PowerStore v1.4.0 (06/18/2021)
New Features/Changes
-Added support for Kubernetes v1.21
-Added support for OpenShift 4.7 with RHEL and CoreOS worker nodes
-Added support for SLES 15.2 and RHEL 8.4 as a host operating system
-Added support for enabling root-squashing for NFS shares
-Added support for disabling snapshot feature during installation
-Added the ability to configure nasName per Storage Class
-Refactored configuration files to use more generic naming

Centos 8.3
Note:
Kubernetes cannot work with Podman (which is now the default container engine for both RHEL and CentOS).
youll need to install the docker engine. 

[root@master ~]# cat /etc/centos-release
CentOS Linux release 8.3.2011
[root@master ~]# uname -a
Linux master 4.18.0-240.el8.x86_64 #1 SMP Fri Sep 25 19:48:47 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

...
...
[root@master ~]# modprobe br_netfilter
[root@master ~]# lsmod |grep -i netfilter
br_netfilter           24576  0
bridge                192512  1 br_netfilter
[root@master ~]# cat > /etc/sysctl.d/k8s.conf <<EOF
> net.bridge.bridge-nf-call-ip6tables = 1
> net.bridge.bridge-nf-call-iptables = 1
> net.ipv4.ip_forward = 1
> EOF
[root@master ~]# sysctl -p /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
net.ipv4.ip_forward = 1

[root@master ~]# cat /etc/hosts
192.168.1.20 master master
192.168.1.21 node1  node1
192.168.1.22 node2  node2
192.168.1.23 node3  node3

[root@master ~]# cat /etc/resolv.conf
# Generated by NetworkManager
nameserver 192.168.1.50

#install docker on master
   45  yum install -y yum-utils device-mapper-persistent-data lvm2 bash-completion git vim net-tools wget
   49  yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
   50  ls /etc/yum.repos.d/
   52  yum makecache
   53  yum list docker-ce | sort -r
   54  yum -y install docker-ce
   55  systemctl enable --now docker
   57  systemctl status docker
   58  docker --version
   Docker version 20.10.7, build f0df350
   59  docker info

#clone master to node1--3

ssh-keygen -t rsa
for i in {master,node1,node2,node3}; do ssh-copy-id $i ; done
cat ~/.ssh/authorized_keys
cat ~/.ssh/id_rsa.pub
for i in {master,node1,node2,node3}; do ssh $i sed -i.bak '/swap/s/^/#/' /etc/fstab ; ssh $i swapoff -a; ssh $i free; done
for i in {node1,node2,node3}; do scp /etc/resolv.conf  $i:/etc/resolv.conf; done
[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i cat /sys/class/dmi/id/product_uuid; done
[root@master ~]# for i in {node1,node2,node3}; do ssh $i modprobe br_netfilter ; scp /etc/sysctl.d/k8s.conf $i:/etc/sysctl.d/k8s.conf; ssh $i sysctl -p /etc/sysctl.d/k8s.conf ; done
[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i systemctl disable firewalld ;  ssh $i systemctl stop firewalld ; done

vi /etc/yum.repos.d/kubernetes.repo
[root@master ~]# cat /etc/yum.repos.d/kubernetes.repo
[kubernetes]
name=Kubernetes
baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-$basearch
enabled=1
gpgcheck=1
repo_gpgcheck=1
gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg
exclude=kubelet kubeadm kubectl

[root@master ~]# for i in {node1,node2,node3}; do scp /etc/yum.repos.d/kubernetes.repo $i:/etc/yum.repos.d/kubernetes.repo ; ssh $i yum repolist ; done
[root@master ~]# yum list --showduplicates kubeadm --disableexcludes=kubernetes | sort -r
[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i yum install -y kubelet-1.20.8-0 kubeadm-1.20.8-0 kubectl-1.20.8-0 --disableexcludes=kubernetes ; done

#rhel8 can't use podman for kubernetes!!!
{
[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i yum module install -y container-tools:2.0  ; done
[root@master ~]# grep -v "^#\|^$" /etc/containers/registries.conf
[registries.search]
registries = ['registry.access.redhat.com', 'registry.redhat.io', 'docker.io']
[registries.insecure]
registries = []
[registries.block]
registries = []
[root@master ~]# podman -v
podman version 1.6.4
[root@master ~]# podman info
host:
  BuildahVersion: 1.12.0-dev
  CgroupVersion: v1
  Conmon:
    package: conmon-2.0.15-1.module_el8.3.0+479+69e2ae26.x86_64
    path: /usr/bin/conmon
    version: 'conmon version 2.0.15, commit: 0198f57f29209da30f765318dda9328bac6a5e07'
  Distribution:
    distribution: '"centos"'
    version: "8"
  MemFree: 6980251648
  MemTotal: 8144797696
  OCIRuntime:
    name: runc
    package: runc-1.0.0-64.rc10.module_el8.3.0+479+69e2ae26.x86_64
    path: /usr/bin/runc
    version: 'runc version spec: 1.0.1-dev'
  SwapFree: 0
  SwapTotal: 0
  arch: amd64
  cpus: 2
  eventlogger: journald
  hostname: master
  kernel: 4.18.0-240.el8.x86_64
  os: linux
  rootless: false
  uptime: 2h 13m 16.45s (Approximately 0.08 days)
registries:
  blocked: null
  insecure: null
  search:
  - registry.access.redhat.com
  - registry.redhat.io
  - docker.io
store:
  ConfigFile: /etc/containers/storage.conf
  ContainerStore:
    number: 0
  GraphDriverName: overlay
  GraphOptions: {}
  GraphRoot: /var/lib/containers/storage
  GraphStatus:
    Backing Filesystem: xfs
    Native Overlay Diff: "true"
    Supports d_type: "true"
    Using metacopy: "false"
  ImageStore:
    number: 0
  RunRoot: /var/run/containers/storage
  VolumePath: /var/lib/containers/storage/volumes

[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i podman pull dellemc/csi-powerstore && podman images ; done
[root@master ~]# podman inspect docker.io/dellemc/csi-powerstore:latest | grep -i "version\|release"
                "release": "291",
                "version": "1.3.0"
        "Version": "",
            "release": "291",
            "version": "1.3.0"


  128  for i in {master,node1,node2,node3}; do ssh $i podman rmi dellemc/csi-powerstore && podman images ; done
  129  for i in {master,node1,node2,node3}; do ssh $i yum module remove -y container-tools:2.0  ; done

[root@master ~]# cat /tmp/config.sh
#/bin/bash
for i in {node1,node2,node3}; do scp /etc/yum.repos.d/kubernetes.repo $i:/etc/yum.repos.d/kubernetes.repo ; ssh $i yum repolist ; done
for i in {master,node1,node2,node3}; do ssh $i setenforce 0 ; ssh $i sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config ; ssh $i getenforce ; done
for i in {master,node1,node2,node3}; do ssh $i systemctl disable firewalld.service; ssh $i systemctl stop firewalld.service ; done
for i in {master,node1,node2,node3}; do ssh $i yum install -y yum-utils device-mapper-persistent-data lvm2 bash-completion git vim net-tools ; done
for i in {master,node1,node2,node3}; do ssh $i yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo ; done
for i in {master,node1,node2,node3}; do ssh $i yum -y install docker-ce ; done
for i in {master,node1,node2,node3}; do ssh $i systemctl enable docker ; ssh $i systemctl start docker ; ssh $i systemctl status docker ; ssh $i docker version; done
for i in {master,node1,node2,node3}; do ssh $i docker --version ; done
for i in {master,node1,node2,node3}; do ssh $i docker pull dellemc/csi-powerstore:v1.3.0 ; ssh $i docker images ; done
for i in {master,node1,node2,node3}; do ssh $i docker images ; done
for i in {master,node1,node2,node3}; do ssh $i yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes ; done
[root@master ~]# chmod +x /tmp/config.sh
[root@master ~]# /tmp/config.sh
}

#Note:
Don't change docker.service to "MountFlags=shared" in /etc/systemd/system/multi-user.target.wants/docker.service, it will cause Centos 8 docker failed to start!!!

[root@master ~]# kubeadm config images pull
I0622 05:26:58.412111    3756 version.go:254] remote version is much newer: v1.21.2; falling back to: stable-1.20
[config/images] Pulled k8s.gcr.io/kube-apiserver:v1.20.8
[config/images] Pulled k8s.gcr.io/kube-controller-manager:v1.20.8
[config/images] Pulled k8s.gcr.io/kube-scheduler:v1.20.8
[config/images] Pulled k8s.gcr.io/kube-proxy:v1.20.8
[config/images] Pulled k8s.gcr.io/pause:3.2
[config/images] Pulled k8s.gcr.io/etcd:3.4.13-0
[config/images] Pulled k8s.gcr.io/coredns:1.7.0
[root@master ~]# docker images
REPOSITORY                           TAG        IMAGE ID       CREATED         SIZE
k8s.gcr.io/kube-proxy                v1.20.8    358e7e6ecf20   5 days ago      127MB
k8s.gcr.io/kube-controller-manager   v1.20.8    c7bd2e859a9e   5 days ago      116MB
k8s.gcr.io/kube-apiserver            v1.20.8    55140e6e050c   5 days ago      122MB
k8s.gcr.io/kube-scheduler            v1.20.8    dcf41923312e   5 days ago      47.3MB
k8s.gcr.io/etcd                      3.4.13-0   0369cf4303ff   9 months ago    253MB
k8s.gcr.io/coredns                   1.7.0      bfe3a36ebd25   12 months ago   45.2MB
k8s.gcr.io/pause                     3.2        80d28bedfe5d   16 months ago   683kB
[root@master ~]#

#add iscsi nic in VC and config each ip
nmcli conn show
nmcli connection modify "Wired connection 1" con-name ens224 ifname ens224
nmcli conn mod ens224 ipv4.address 192.168.2.220/24 ipv4.method static connection.autoconnect yes && nmcli con up ens224 && nmcli con show

  138  kubeadm init --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=192.168.1.20
  139  for i in {node1,node2,node3}; do ssh $i kubeadm join 192.168.1.20:6443 --token tkmf3y.x3rzt3m6rtxhwc02 --discovery-token-ca-cert-hash sha256:9823a831616f82a964ae1116b6c6dfeac5c83e865a7205230adc8a0828e18fb8 ; done
  140  systemctl status kubelet
  141  netstat -ntpl
  142  kubectl get nodes
  144  echo "export KUBECONFIG=/etc/kubernetes/admin.conf" >> ~/.bash_profile
  145  echo "source <(kubectl completion bash)" >> ~/.bash_profile
  146  source ~/.bash_profile
  147  kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml
  148  kubectl get cs
  149  vi ~/.vimrc
  150  vim /etc/kubernetes/manifests/kube-controller-manager.yaml     //delete --port=0 line!!
  151  vim /etc/kubernetes/manifests/kube-scheduler.yaml
  152  kubectl get cs
  153  kubectl cluster-info
  154  kubectl get nodes
[root@master ~]# kubectl get nodes
NAME     STATUS   ROLES                  AGE     VERSION
master   Ready    control-plane,master   12m   v1.20.8
node1    Ready    <none>                 10m   v1.20.8
node2    Ready    <none>                 10m   v1.20.8
node3    Ready    <none>                 10m   v1.20.8
[root@master ~]# kubectl get pods -A
NAMESPACE     NAME                             READY   STATUS    RESTARTS   AGE
kube-system   coredns-74ff55c5b-hj952          1/1     Running   1          26m
kube-system   coredns-74ff55c5b-qfg5p          1/1     Running   1          26m
kube-system   etcd-master                      1/1     Running   1          26m
kube-system   kube-apiserver-master            1/1     Running   1          26m
kube-system   kube-controller-manager-master   1/1     Running   1          17m
kube-system   kube-flannel-ds-amd64-5lgwk      1/1     Running   1          21m
kube-system   kube-flannel-ds-amd64-fvtgw      1/1     Running   1          21m
kube-system   kube-flannel-ds-amd64-hcgth      1/1     Running   1          21m
kube-system   kube-flannel-ds-amd64-rgmkv      1/1     Running   1          21m
kube-system   kube-proxy-kc7dp                 1/1     Running   1          24m
kube-system   kube-proxy-nwm98                 1/1     Running   1          24m
kube-system   kube-proxy-vl27z                 1/1     Running   1          24m
kube-system   kube-proxy-xmlqj                 1/1     Running   1          26m
kube-system   kube-scheduler-master            1/1     Running   1          17m

#install helm
[root@master ~]# curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash
[root@master ~]# helm version
version.BuildInfo{Version:"v3.6.1", GitCommit:"61d8e8c4a6f95540c15c6a65f36a6dd0a45e7a2f", GitTreeState:"clean", GoVersion:"go1.16.5"}

#install iscsi
[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i yum -y install iscsi-initiator-utils device-mapper-multipath lsscsi ; done
[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i cat /etc/iscsi/initiatorname.iscsi ; done
InitiatorName=iqn.1994-05.com.redhat:master
InitiatorName=iqn.1994-05.com.redhat:node1
InitiatorName=iqn.1994-05.com.redhat:node2
InitiatorName=iqn.1994-05.com.redhat:node3
[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i iscsiadm -m discovery -t st -p 192.168.2.30:3260 ; ssh $i iscsiadm -m node -L all ; ssh $i hostname ; ssh $i iscsiadm -m session ; done
[root@master ~]# for i in {master,node1,node2,node3}; do ssh $i mpathconf --enable --user_friendly_names y --with_module y --with_multipathd y --find_multipaths y ; ssh $i systemctl is-enabled multipathd.service ; ssh $i systemctl start multipathd.service ; ssh $i hostname; ssh $i systemctl status multipathd.service ; done


#install powerstore csi gitclone
Procedure
Run git clone https://github.com/dell/csi-powerstore.git to clone the git repository
...
...
[root@master ~]# cd csi-powerstore/
[root@master csi-powerstore]# ls
cmd  core  dell-csi-helm-installer  docker-files  docker.mk  env.sh  go.mod  go.sum  helm  LICENSE  licenses  Makefile  mocks  pkg  README.md  tests
[root@master csi-powerstore]# ls helm/
config.yaml  csi-powerstore  README.md  samples  secret.yaml

[root@master csi-powerstore]# cat helm/csi-powerstore/driver-image.yaml | grep driver
  # "images.driver" defines the container images used for the driver container.
  driver: dellemc/csi-powerstore:v1.4.0
[root@master csi-powerstore]# for i in {master,node1,node2,node3}; do ssh $i docker pull dellemc/csi-powerstore:v1.4.0 ; ssh $i docker images ; done

[root@master dell-csi-helm-installer]# kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshots.yaml
customresourcedefinition.apiextensions.k8s.io/volumesnapshots.snapshot.storage.k8s.io created
[root@master dell-csi-helm-installer]# kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotclasses.yaml
customresourcedefinition.apiextensions.k8s.io/volumesnapshotclasses.snapshot.storage.k8s.io created
[root@master dell-csi-helm-installer]# kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/client/config/crd/snapshot.storage.k8s.io_volumesnapshotcontents.yaml
customresourcedefinition.apiextensions.k8s.io/volumesnapshotcontents.snapshot.storage.k8s.io created
[root@master dell-csi-helm-installer]# kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/deploy/kubernetes/snapshot-controller/rbac-snapshot-controller.yaml
serviceaccount/snapshot-controller created
clusterrole.rbac.authorization.k8s.io/snapshot-controller-runner created
clusterrolebinding.rbac.authorization.k8s.io/snapshot-controller-role created
role.rbac.authorization.k8s.io/snapshot-controller-leaderelection created
rolebinding.rbac.authorization.k8s.io/snapshot-controller-leaderelection created
[root@master dell-csi-helm-installer]# kubectl apply -f https://raw.githubusercontent.com/kubernetes-csi/external-snapshotter/v4.0.0/deploy/kubernetes/snapshot-controller/setup-snapshot-controller.yaml
statefulset.apps/snapshot-controller created

[root@master dell-csi-helm-installer]# kubectl get all --all-namespaces
NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE
default       pod/snapshot-controller-0            1/1     Running   0          52s
kube-system   pod/coredns-74ff55c5b-8hbq8          0/1     Running   0          99m
kube-system   pod/coredns-74ff55c5b-msrrz          0/1     Running   0          99m
kube-system   pod/etcd-master                      1/1     Running   0          99m
kube-system   pod/kube-apiserver-master            1/1     Running   0          99m
kube-system   pod/kube-controller-manager-master   1/1     Running   0          90m
kube-system   pod/kube-flannel-ds-amd64-b6vd7      1/1     Running   0          94m
kube-system   pod/kube-flannel-ds-amd64-dws65      1/1     Running   0          94m
kube-system   pod/kube-flannel-ds-amd64-tqqnj      1/1     Running   0          94m
kube-system   pod/kube-flannel-ds-amd64-xtcjv      1/1     Running   0          94m
kube-system   pod/kube-proxy-5lbwp                 1/1     Running   0          98m
kube-system   pod/kube-proxy-b5n8s                 1/1     Running   0          97m
kube-system   pod/kube-proxy-c6jzr                 1/1     Running   0          98m
kube-system   pod/kube-proxy-r7kxj                 1/1     Running   0          99m
kube-system   pod/kube-scheduler-master            1/1     Running   0          90m

NAMESPACE     NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
default       service/kubernetes   ClusterIP   10.96.0.1    <none>        443/TCP                  99m
kube-system   service/kube-dns     ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   99m

NAMESPACE     NAME                                     DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   daemonset.apps/kube-flannel-ds-amd64     4         4         4       4            4           <none>                   94m
kube-system   daemonset.apps/kube-flannel-ds-arm       0         0         0       0            0           <none>                   94m
kube-system   daemonset.apps/kube-flannel-ds-arm64     0         0         0       0            0           <none>                   94m
kube-system   daemonset.apps/kube-flannel-ds-ppc64le   0         0         0       0            0           <none>                   94m
kube-system   daemonset.apps/kube-flannel-ds-s390x     0         0         0       0            0           <none>                   94m
kube-system   daemonset.apps/kube-proxy                4         4         4       4            4           kubernetes.io/os=linux   99m

NAMESPACE     NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
kube-system   deployment.apps/coredns   0/2     2            0           99m

NAMESPACE     NAME                                DESIRED   CURRENT   READY   AGE
kube-system   replicaset.apps/coredns-74ff55c5b   2         2         0       99m

NAMESPACE   NAME                                   READY   AGE
default     statefulset.apps/snapshot-controller   1/1     52s
[root@master csi-powerstore]# kubectl logs snapshot-controller-0 -n default
I0324 11:43:42.898334       1 main.go:71] Version: v4.0.0
I0324 11:43:42.900077       1 main.go:120] Start NewCSISnapshotController with kubeconfig [] resyncPeriod [15m0s]
I0324 11:43:42.900962       1 reflector.go:219] Starting reflector *v1.VolumeSnapshot (15m0s) from github.com/kubernetes-csi/external-snapshotter/client/v4/informers/externalversions/factory.go:117
I0324 11:43:42.900980       1 reflector.go:255] Listing and watching *v1.VolumeSnapshot from github.com/kubernetes-csi/external-snapshotter/client/v4/informers/externalversions/factory.go:117
I0324 11:43:42.901651       1 reflector.go:219] Starting reflector *v1.VolumeSnapshotClass (15m0s) from github.com/kubernetes-csi/external-snapshotter/client/v4/informers/externalversions/factory.go:117
I0324 11:43:42.901690       1 reflector.go:255] Listing and watching *v1.VolumeSnapshotClass from github.com/kubernetes-csi/external-snapshotter/client/v4/informers/externalversions/factory.go:117
I0324 11:43:42.901961       1 reflector.go:219] Starting reflector *v1.VolumeSnapshotContent (15m0s) from github.com/kubernetes-csi/external-snapshotter/client/v4/informers/externalversions/factory.go:117
I0324 11:43:42.901999       1 reflector.go:255] Listing and watching *v1.VolumeSnapshotContent from github.com/kubernetes-csi/external-snapshotter/client/v4/informers/externalversions/factory.go:117
I0324 11:43:42.902305       1 reflector.go:219] Starting reflector *v1.PersistentVolumeClaim (15m0s) from k8s.io/client-go/informers/factory.go:134
I0324 11:43:42.902314       1 reflector.go:255] Listing and watching *v1.PersistentVolumeClaim from k8s.io/client-go/informers/factory.go:134
I0324 11:43:42.902533       1 snapshot_controller_base.go:133] Starting snapshot controller
I0324 11:43:43.002825       1 shared_informer.go:270] caches populated
I0324 11:43:43.002921       1 snapshot_controller_base.go:485] controller initialized
[root@master csi-powerstore]#

[root@master csi-powerstore]# kubectl create namespace csi-powerstore

[root@master csi-powerstore]# cat helm/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: powerstore-config
  # Set driver namespace
  namespace: csi-powerstore
type: Opaque
data:
  config: CONFIG_YAML
 
[root@linux17 ~]# curl -k --user admin:P@ssw0rd! https://192.168.1.30/api/rest/cluster?select=* | json_reformat
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   310    0   310    0     0   1920      0 --:--:-- --:--:-- --:--:--  1913
[
    {
        "id": "0",
        "global_id": "PS04e694d0c442",
        "name": "POD3-Cluster1",
        "physical_mtu": 1500,
        "master_appliance_id": "A1",
        "state": "Configured",
        "appliance_count": 1,
        "management_address": "192.168.1.30",
        "is_encryption_enabled": true,
        "storage_discovery_address": "192.168.2.30",
        "compatibility_level": 1,
        "state_l10n": "Configured"
    }
]
 
[root@master csi-powerstore]# cat helm/config.yaml
# You can apply current config to Kubernetes cluster by running following command:
#
# sed "s/CONFIG_YAML/`cat config.yaml | base64 -w0`/g" secret.yaml | kubectl apply -f -
#
arrays:
  - endpoint: "https://192.168.1.30/api/rest"
    # full URL path to the PowerStore API
    globalID: "PS04e694d0c442"
    # unique id of the PowerStore array, powerstore os 2.0 add!!!
    username: "admin"
    # username for connecting to API
    password: "P@ssw0rd!"
    # password for connecting to API
    skipCertificateValidation: true
    # indicates if client side validation of (management)server's certificate can be skipped
    isDefault: true
    # treat current array as a default (would be used by storage classes without arrayID parameter)
    blockProtocol: "auto"
    # what SCSI transport protocol use on node side (FC, ISCSI, None, or auto)
    #nasName: "nas-server"
    # what NAS should be used for NFS volumes
[root@master csi-powerstore]#

#csi v1.4.0 storageclass use arrayID!!
[root@master csi-powerstore]# cat helm/samples/storageclass/powerstore-xfs.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: powerstore-xfs
provisioner: csi-powerstore.dellemc.com
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
parameters:
  arrayID: "PS04e694d0c442"
  FsType: xfs

#csi v1.3.0 storageclass use arrayIP!!
[root@master csi-powerstore]# cat helm/samples/storageclass/powerstore-xfs.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: powerstore-xfs
provisioner: csi-powerstore.dellemc.com
reclaimPolicy: Delete
allowVolumeExpansion: true
volumeBindingMode: Immediate
parameters:
  arrayIP: "192.168.1.30"
  FsType: xfs
  
[root@master csi-powerstore]# kubectl get sc
No resources found
[root@master csi-powerstore]# kubectl apply -f helm/samples/storageclass/powerstore-xfs.yaml
storageclass.storage.k8s.io/powerstore-xfs created
[root@master csi-powerstore]# kubectl get sc
NAME             PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
powerstore-xfs   csi-powerstore.dellemc.com   Delete          Immediate           true                   18s

[root@master csi-powerstore]# sed "s/CONFIG_YAML/`cat helm/config.yaml | base64 -w0`/g" helm/secret.yaml | kubectl apply -f -
[root@master csi-powerstore]# kubectl get secrets -n csi-powerstore
NAME                  TYPE                                  DATA   AGE
default-token-9zbgd   kubernetes.io/service-account-token   3      81m
powerstore-config     Opaque                                1      12s

[root@master csi-powerstore]# cd dell-csi-helm-installer && cp ../helm/csi-powerstore/values.yaml ./my-powerstore-settings.yaml
[root@master dell-csi-helm-installer]#
[root@master dell-csi-helm-installer]# vim my-powerstore-settings.yaml
[root@master dell-csi-helm-installer]# grep -v "^#\|^$\|^  #" my-powerstore-settings.yaml
driverName: "csi-powerstore.dellemc.com"
volumeNamePrefix: csi
nodeNamePrefix: csi-node
nodeIDPath: /etc/hostname
controller:
  nodeSelector:
  tolerations:
  replicas: 3
node:
  nodeSelector:
  tolerations:
connection:
  enableCHAP: false
nodeFCPortsFilterFile: /etc/fc-ports-filter
snapshot:
  enabled: false
replication:
  enabled: false
externalAccess:

[root@master dell-csi-helm-installer]#

[root@master dell-csi-helm-installer]# ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml
------------------------------------------------------
> Installing CSI Driver: csi-powerstore on 1.20
------------------------------------------------------
------------------------------------------------------
> Checking to see if CSI Driver is already installed
------------------------------------------------------
------------------------------------------------------
> Verifying Kubernetes and driver configuration
------------------------------------------------------
|- Kubernetes Version: 1.20
|
|- Driver: csi-powerstore
|
|- Verifying Kubernetes versions
  |
  |--> Verifying minimum Kubernetes version                         Success
  |
  |--> Verifying maximum Kubernetes version                         Success
|
|- Verifying that required namespaces have been created             Success
|
|- Verifying that required secrets have been created                Success
|
|- Verifying alpha snapshot resources
  |
  |--> Verifying that alpha snapshot CRDs are not installed         Success
|
|- Verifying iSCSI installation                                     Success
|
|- Verifying helm version                                           Success

------------------------------------------------------
> Verification Complete - Success
------------------------------------------------------
|
|- Installing Driver                                                Success
  |
  |--> Waiting for Deployment powerstore-controller to be ready     Success
  |
  |--> Waiting for DaemonSet powerstore-node to be ready            Success
------------------------------------------------------
> Operation complete
------------------------------------------------------
[root@master dell-csi-helm-installer]#

[root@master dell-csi-helm-installer]# kubectl get pods -n csi-powerstore
NAME                                     READY   STATUS    RESTARTS   AGE
powerstore-controller-685ff6cb4b-2fr66   4/4     Running   0          7m11s
powerstore-controller-685ff6cb4b-b5ds9   4/4     Running   0          7m11s
powerstore-controller-685ff6cb4b-hzfxh   4/4     Running   0          7m11s
powerstore-node-qbkv4                    2/2     Running   0          7m12s
powerstore-node-vmbnt                    2/2     Running   0          7m12s
powerstore-node-whxgc                    2/2     Running   0          7m12s
[root@master dell-csi-helm-installer]#

[root@master dell-csi-helm-installer]# kubectl describe -n csi-powerstore pods powerstore-controller-685ff6cb4b-2fr66
Name:         powerstore-controller-685ff6cb4b-2fr66
Namespace:    csi-powerstore
Priority:     0
Node:         node1/192.168.1.21
Start Time:   Tue, 22 Jun 2021 11:28:04 -0400
Labels:       name=powerstore-controller
              pod-template-hash=685ff6cb4b
Annotations:  <none>
Status:       Running
IP:           10.244.1.3
IPs:
  IP:           10.244.1.3
Controlled By:  ReplicaSet/powerstore-controller-685ff6cb4b
Containers:
  attacher:
    Container ID:  docker://3826029288cde7cd96e827fc3a0ac7346b3e2e56a40f051ba8b1993f8ff1c968
    Image:         k8s.gcr.io/sig-storage/csi-attacher:v3.2.1
    Image ID:      docker-pullable://k8s.gcr.io/sig-storage/csi-attacher@sha256:60ab9b3e6a030d3038c87c0d6bca2930f58d1d72823e6a4af09767dc83b696a2
    Port:          <none>
    Host Port:     <none>
    Args:
      --csi-address=$(ADDRESS)
      --v=5
      --leader-election
      --worker-threads=130
      --resync=10s
      --timeout=130s
    State:          Running
      Started:      Tue, 22 Jun 2021 11:28:23 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      ADDRESS:  /var/run/csi/csi.sock
    Mounts:
      /var/run/csi from socket-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from powerstore-controller-token-dbsxh (ro)
  resizer:
    Container ID:  docker://7d1b09782c79bc5b85b8c15434792db93a1402a308213dffe24b5c46a602d8dc
    Image:         k8s.gcr.io/sig-storage/csi-resizer:v1.2.0
    Image ID:      docker-pullable://k8s.gcr.io/sig-storage/csi-resizer@sha256:36c31f7e1f433c9634d24f876353e8646246d81a03c4e351202c2644daff1620
    Port:          <none>
    Host Port:     <none>
    Args:
      --csi-address=$(ADDRESS)
      --v=5
      --leader-election
    State:          Running
      Started:      Tue, 22 Jun 2021 11:28:42 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      ADDRESS:  /var/run/csi/csi.sock
    Mounts:
      /var/run/csi from socket-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from powerstore-controller-token-dbsxh (ro)
  provisioner:
    Container ID:  docker://1e8a78c2527d04a71eb98f9ed833ddb9225233d65cacc6171007e50267e4a512
    Image:         k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1
    Image ID:      docker-pullable://k8s.gcr.io/sig-storage/csi-provisioner@sha256:4e74c0492bceddc598de1c90cc5bc14dcda94cb49fa9c5bad9d117c4834b5e08
    Port:          <none>
    Host Port:     <none>
    Args:
      --csi-address=$(ADDRESS)
      --volume-name-prefix=csi
      --volume-name-uuid-length=10
      --v=5
      --leader-election
      --default-fstype=ext4
      --extra-create-metadata
      --feature-gates=Topology=true
    State:          Running
      Started:      Tue, 22 Jun 2021 11:28:56 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      ADDRESS:  /var/run/csi/csi.sock
    Mounts:
      /var/run/csi from socket-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from powerstore-controller-token-dbsxh (ro)
  driver:
    Container ID:  docker://65ea039a4bb5b0ee1a2d9fc29d2737d067af2bd3af1fdab0d50e0d209a8349fc
    Image:         dellemc/csi-powerstore:v1.4.0
    Image ID:      docker-pullable://dellemc/csi-powerstore@sha256:d98c918ac4c40839b67e379fb0711c55bf4ca1c4c7355b323723a29ad8a9bce4
    Port:          <none>
    Host Port:     <none>
    Command:
      /csi-powerstore
    State:          Running
      Started:      Tue, 22 Jun 2021 11:28:57 -0400
    Ready:          True
    Restart Count:  0
    Environment:
      CSI_ENDPOINT:                      /var/run/csi/csi.sock
      X_CSI_MODE:                        controller
      X_CSI_DEBUG:                       true
      X_CSI_DRIVER_NAME:                 csi-powerstore.dellemc.com
      X_CSI_POWERSTORE_EXTERNAL_ACCESS:
      X_CSI_POWERSTORE_CONFIG_PATH:      /powerstore-config/config
      GOPOWERSTORE_DEBUG:                true
    Mounts:
      /powerstore-config from powerstore-config (rw)
      /var/run/csi from socket-dir (rw)
      /var/run/secrets/kubernetes.io/serviceaccount from powerstore-controller-token-dbsxh (ro)
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  socket-dir:
    Type:       EmptyDir (a temporary directory that shares a pod's lifetime)
    Medium:
    SizeLimit:  <unset>
  powerstore-config:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  powerstore-config
    Optional:    false
  powerstore-controller-token-dbsxh:
    Type:        Secret (a volume populated by a Secret)
    SecretName:  powerstore-controller-token-dbsxh
    Optional:    false
QoS Class:       BestEffort
Node-Selectors:  <none>
Tolerations:     node.kubernetes.io/not-ready:NoExecute op=Exists for 300s
                 node.kubernetes.io/unreachable:NoExecute op=Exists for 300s
Events:
  Type    Reason     Age    From               Message
  ----    ------     ----   ----               -------
  Normal  Scheduled  8m52s  default-scheduler  Successfully assigned csi-powerstore/powerstore-controller-685ff6cb4b-2fr66 to node1
  Normal  Pulling    8m51s  kubelet            Pulling image "k8s.gcr.io/sig-storage/csi-attacher:v3.2.1"
  Normal  Pulled     8m40s  kubelet            Successfully pulled image "k8s.gcr.io/sig-storage/csi-attacher:v3.2.1" in 11.581288208s
  Normal  Created    8m33s  kubelet            Created container attacher
  Normal  Started    8m33s  kubelet            Started container attacher
  Normal  Pulling    8m33s  kubelet            Pulling image "k8s.gcr.io/sig-storage/csi-resizer:v1.2.0"
  Normal  Pulled     8m16s  kubelet            Successfully pulled image "k8s.gcr.io/sig-storage/csi-resizer:v1.2.0" in 17.25039299s
  Normal  Pulling    8m14s  kubelet            Pulling image "k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1"
  Normal  Created    8m14s  kubelet            Created container resizer
  Normal  Started    8m14s  kubelet            Started container resizer
  Normal  Pulled     8m2s   kubelet            Successfully pulled image "k8s.gcr.io/sig-storage/csi-provisioner:v2.2.1" in 11.514040919s
  Normal  Created    8m     kubelet            Created container provisioner
  Normal  Started    8m     kubelet            Started container provisioner
  Normal  Pulling    8m     kubelet            Pulling image "dellemc/csi-powerstore:v1.4.0"
  Normal  Pulled     7m59s  kubelet            Successfully pulled image "dellemc/csi-powerstore:v1.4.0" in 390.546985ms
  Normal  Created    7m59s  kubelet            Created container driver
  Normal  Started    7m59s  kubelet            Started container driver
[root@master dell-csi-helm-installer]#

[root@master dell-csi-helm-installer]# kubectl get sc
NAME             PROVISIONER                  RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE
powerstore-xfs   csi-powerstore.dellemc.com   Delete          Immediate           true                   57m
[root@master dell-csi-helm-installer]# kubectl describe sc powerstore-xfs
Name:            powerstore-xfs
IsDefaultClass:  No
Annotations:     kubectl.kubernetes.io/last-applied-configuration={"allowVolumeExpansion":true,"apiVersion":"storage.k8s.io/v1","kind":"StorageClass","metadata":{"annotations":{},"name":"powerstore-xfs"},"parameters":{"FsType":"xfs","arrayID":"PS04e694d0c442"},"provisioner":"csi-powerstore.dellemc.com","reclaimPolicy":"Delete","volumeBindingMode":"Immediate"}

Provisioner:           csi-powerstore.dellemc.com
Parameters:            FsType=xfs,arrayID=PS04e694d0c442
AllowVolumeExpansion:  True
MountOptions:          <none>
ReclaimPolicy:         Delete
VolumeBindingMode:     Immediate
Events:                <none>
[root@master dell-csi-helm-installer]#

[root@master tmp]# yum localinstall pstcli-1.0.0.227.x86_64.release_5H1P8.rpm
[root@master tmp]# pstcli -d 192.168.1.30 -u admin -p P@ssw0rd! -session
cli> host show -sort name+ -limit 3
 #  |                  id                  |            name             |   description   | os_type | host_group.name
----+--------------------------------------+-----------------------------+-----------------+---------+-----------------
  1 | 37e0b151-e866-4b69-854c-0682324bdce7 | csi-node-node1-192.168.1.21 | k8s node: node1 | Linux   |
  2 | cddbf404-e7de-41b5-8252-c5bfaebe9373 | csi-node-node2-192.168.1.22 | k8s node: node2 | Linux   |
  3 | e1431128-bc5b-4f9e-bf15-014fef2d84d7 | csi-node-node3-192.168.1.23 | k8s node: node3 | Linux   |


##SQL test
for i in {node1,node2,node3}; do ssh $i docker pull mcr.microsoft.com/mssql/rhel/server:2019-latest ; ssh $i docker images ; done
REPOSITORY                                         TAG           IMAGE ID       CREATED         SIZE
dellemc/csi-powerstore                             v1.4.0        8bddb808e6ec   4 days ago      253MB
k8s.gcr.io/kube-proxy                              v1.20.8       358e7e6ecf20   6 days ago      127MB
mcr.microsoft.com/mssql/rhel/server                2019-latest   31312494c1b6   3 weeks ago     1.54GB
quay.io/coreos/flannel                             v0.14.0       8522d622299c   5 weeks ago     67.9MB
k8s.gcr.io/sig-storage/csi-provisioner             v2.2.1        b93fb1bfc551   6 weeks ago     56.4MB
k8s.gcr.io/sig-storage/csi-attacher                v3.2.1        6de272f18137   6 weeks ago     53.5MB
k8s.gcr.io/sig-storage/csi-resizer                 v1.2.0        0aa9629e1508   6 weeks ago     54MB
k8s.gcr.io/sig-storage/csi-node-driver-registrar   v2.2.0        8e5a15e16dca   8 weeks ago     18.7MB
k8s.gcr.io/coredns                                 1.7.0         bfe3a36ebd25   12 months ago   45.2MB
k8s.gcr.io/pause                                   3.2           80d28bedfe5d   16 months ago   683kB

[root@master tmp]# kubectl create secret generic mssql --from-literal=SA_PASSWORD="P@ssw0rd!"
secret/mssql created
[root@master tmp]# kubectl get secrets | grep -i mssql
mssql                               Opaque                                1      26s

[root@master tmp]# mkdir sql
[root@master tmp]# wget https://raw.githubusercontent.com/HendreyFu/csi/main/pvc.yaml -O /tmp/sql/pvc.yaml
[root@master tmp]# wget https://raw.githubusercontent.com/HendreyFu/csi/main/sql.yaml -O /tmp/sql/sql.yaml
[root@master tmp]# vim sql/pvc.yaml
[root@master tmp]# kubectl create -f sql/pvc.yaml
persistentvolumeclaim/mssql-data created
persistentvolumeclaim/mssql-data2 created
persistentvolumeclaim/mssql-log2 created
[root@master tmp]# kubectl get pvc
NAME          STATUS   VOLUME           CAPACITY   ACCESS MODES   STORAGECLASS     AGE
mssql-data    Bound    csi-2eb515d932   10Gi       RWO            powerstore-xfs   4s
mssql-data2   Bound    csi-88d9b52745   5Gi        RWO            powerstore-xfs   4s
mssql-log2    Bound    csi-c8c433a697   2Gi        RWO            powerstore-xfs   4s
[root@master tmp]# kubectl get pv
NAME             CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                        STORAGECLASS     REASON   AGE
csi-2eb515d932   10Gi       RWO            Delete           Bound    csi-powerstore/mssql-data    powerstore-xfs            54s
csi-88d9b52745   5Gi        RWO            Delete           Bound    csi-powerstore/mssql-data2   powerstore-xfs            54s
csi-c8c433a697   2Gi        RWO            Delete           Bound    csi-powerstore/mssql-log2    powerstore-xfs            54s
[root@master tmp]# kubectl create secret generic mssql --from-literal=SA_PASSWORD="P@ssw0rd!"
[root@master tmp]# vim sql/sql.yaml
[root@master tmp]# kubectl create -f sql/sql.yaml
deployment.apps/mssql-deployment created
service/mssql-deployment created

[root@master tmp]# kubectl get pods
NAME                                     READY   STATUS    RESTARTS   AGE
mssql-deployment-5f4cd94964-bw9gt        1/1     Running   0          113m
powerstore-controller-7bc455d5c7-g2hlg   5/5     Running   4          27h
powerstore-controller-7bc455d5c7-njnzn   5/5     Running   1          27h
powerstore-controller-7bc455d5c7-w6fsl   5/5     Running   3          27h
powerstore-node-74zc8                    2/2     Running   0          27h
powerstore-node-7n9wp                    2/2     Running   0          27h
powerstore-node-k24jj                    2/2     Running   0          27h

[root@master tmp]# kubectl logs mssql-deployment-5f4cd94964-bw9gt -f
[root@master tmp]# kubectl get services
NAME               TYPE       CLUSTER-IP      EXTERNAL-IP   PORT(S)          AGE
mssql-deployment   NodePort   10.96.225.232   <none>        1433:30415/TCP   45s
[root@master tmp]# kubectl get pods -o wide
NAME                                     READY   STATUS    RESTARTS   AGE   IP             NODE    NOMINATED NODE   READINESS GATES
mssql-deployment-5f4cd94964-bw9gt        1/1     Running   0          61s   10.244.2.6     node2   <none>           <none>
powerstore-controller-7bc455d5c7-g2hlg   5/5     Running   4          25h   10.244.1.5     node1   <none>           <none>
powerstore-controller-7bc455d5c7-njnzn   5/5     Running   1          25h   10.244.3.4     node3   <none>           <none>
powerstore-controller-7bc455d5c7-w6fsl   5/5     Running   3          25h   10.244.2.3     node2   <none>           <none>
powerstore-node-74zc8                    2/2     Running   0          25h   192.168.1.22   node2   <none>           <none>
powerstore-node-7n9wp                    2/2     Running   0          25h   192.168.1.23   node3   <none>           <none>
powerstore-node-k24jj                    2/2     Running   0          25h   192.168.1.21   node1   <none>           <none>
[root@master tmp]# kubectl exec -it mssql-deployment-5f4cd94964-bw9gt bash
kubectl exec [POD] [COMMAND] is DEPRECATED and will be removed in a future version. Use kubectl exec [POD] -- [COMMAND] instead.
bash-4.4$ /opt/mssql-tools/bin/sqlcmd -S localhost,1433 -U sa -P P@ssw0rd!
1> select @@VERSION as Version, SERVERPROPERTY('ServerName') as 'Container ID', SERVERPROPERTY('Edition') as Edition;
2> go
1> create database [TestDB] CONTAINMENT = NONE on primary ( NAME = N'TestDB', FILENAME = N'/var/opt/mssql/data2/TestDB.mdf', SIZE = 4GB, FILEGROWTH = 1024KB ) LOG ON ( NAME = N'TestDB_log', FILENAME = N'/var/opt/mssql/log2/TestDB_log.ldf', SIZE = 1GB, FILEGROWTH = 10% )
2> go
1> use TestDB
2> CREATE TABLE Inventory (id INT, name NVARCHAR(50), quantity INT)
3> INSERT INTO Inventory VALUES (1, 'banana', 150); INSERT INTO Inventory VALUES (2, 'orange', 154);
4> go
Changed database context to 'TestDB'.

(1 rows affected)

(1 rows affected)
1> SELECT * FROM Inventory WHERE quantity > 152;
2> go
id          name                                               quantity
----------- -------------------------------------------------- -----------
          2 orange                                                     154

(1 rows affected)
1>exit

bash-4.4$ cd /var/opt/mssql/
bash-4.4$ ls
data  data2  log  log2  secrets
bash-4.4$ ls -hl data2/
total 4.1G
-rw-r-----. 1 mssql 10001 4.0G Jun 13 04:11 TestDB.mdf
bash-4.4$ ls -hl log2/
total 1.0G
-rw-r-----. 1 mssql 10001 1.0G Jun 13 04:13 TestDB_log.ldf
bash-4.4$ df -H
Filesystem                                                                                                     Size  Used Avail Use% Mounted on
overlay                                                                                                         67G  5.6G   62G   9% /
tmpfs                                                                                                           68M     0   68M   0% /dev
tmpfs                                                                                                          4.1G     0  4.1G   0% /sys/fs/cgroup
/dev/mapper/cl-root                                                                                             67G  5.6G   62G   9% /etc/hosts
shm                                                                                                             68M     0   68M   0% /dev/shm
/var/lib/kubelet/plugins/kubernetes.io/csi/pv/csi-dd3ec0be94/globalmount/3d04bbd4-3605-4587-a7ae-bf19bf22de5b   11G  122M   11G   2% /var/opt/mssql
/var/lib/kubelet/plugins/kubernetes.io/csi/pv/csi-7edc4ccc2d/globalmount/c9fc234b-3340-4949-8e73-a5b661fcd448  5.4G  4.4G  1.1G  81% /var/opt/mssql/data2
/var/lib/kubelet/plugins/kubernetes.io/csi/pv/csi-b9e32a1ef8/globalmount/696cb268-651c-4b64-95fc-21d649880944  2.2G  1.2G  1.1G  52% /var/opt/mssql/log2
tmpfs                                                                                                          4.1G   13k  4.1G   1% /run/secrets/kubernetes.io/serviceaccount
tmpfs                                                                                                          4.1G     0  4.1G   0% /proc/acpi
tmpfs                                                                                                          4.1G     0  4.1G   0% /proc/scsi
tmpfs                                                                                                          4.1G     0  4.1G   0% /sys/firmware
bash-4.4$
bash-4.4$ exit
exit
[root@master tmp]#

#install Azure Data Studio
1)https://docs.microsoft.com/en-us/sql/azure-data-studio/download-azure-data-studio?view=sql-server-ver15
2)check witch node and ports run sql server
[root@master tmp]# kubectl get service -o wide
NAME               TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE     SELECTOR
mssql-deployment   NodePort   10.110.1.211   <none>        1433:30859/TCP   5h10m   app=mssql
[root@master tmp]# kubectl get pods -o wide
NAME                                     READY   STATUS    RESTARTS   AGE     IP             NODE     NOMINATED NODE   READINESS GATES
mssql-deployment-5f4cd94964-hdbrd        1/1     Running   0          5h10m   10.244.1.12    node1    <none>           <none>
powerstore-controller-7bc455d5c7-5zm85   5/5     Running   112        14h     10.244.1.11    node1    <none>           <none>
powerstore-controller-7bc455d5c7-kqlrv   5/5     Running   10         14h     10.244.0.10    master   <none>           <none>
powerstore-node-rskw4                    2/2     Running   4          14h     192.168.1.21   node1    <none>           <none>
powerstore-node-z7l8b                    2/2     Running   4          14h     192.168.1.20   master   <none>           <none>
[root@master tmp]#
3)check sql listen port
[root@master tmp]# ssh node1
Last login: Sun Jun 13 02:22:10 2021 from 192.168.1.20
[root@node1 ~]# netstat -ntpl|grep 30859
tcp        0      0 0.0.0.0:30859           0.0.0.0:*               LISTEN      5555/kube-proxy
4)install GUI Azure Data Studio on client,
Server:    192.168.1.21,30859
User Name: sa
Password:  P@ssw0rd!
5)test db scripts...

#use root config sql container
1)
[root@node1 ~]# docker ps -l
CONTAINER ID   IMAGE          COMMAND                  CREATED       STATUS       PORTS     NAMES
efcf6d8e2a05   31312494c1b6   "/opt/mssql/bin/perm…"   7 hours ago   Up 7 hours             k8s_mssql_mssql-deployment-5f4cd94964-hdbrd_csi-powerstore_526d50ff-b729-49d4-ae20-d1caa22ae68c_0
[root@node1 ~]# docker exec -u 0 -it efcf6d8e2a05 bash
[root@mssqlinst /]# id
uid=0(root) gid=0(root) groups=0(root),10001
[root@mssqlinst /]# /opt/mssql/bin/
checkinstallextensibility.sh  generate-sql-dump.sh          mssql-conf                    setnetbr
compress-dump.sh              handle-crash.sh               paldumper                     sqlservr
crash-support-functions.sh    launchpadd                    permissions_check.sh
[root@mssqlinst /]# /opt/mssql/bin/mssql-conf list
control.alternatewritethrough                           Enable optimized write through flush for O_DSYNC requests
control.hestacksize                                     Host extension stack size in KB
control.stoponguestprocessfault                         Stops the process if any guest process reports unhandled exception
control.writethrough                                    Use O_DSYNC for file flag write through requests
[root@mssqlinst /]# /opt/mssql/bin/mssql-conf setup
The license terms for this product can be found in
/usr/share/doc/mssql-server or downloaded from:
https://go.microsoft.com/fwlink/?LinkId=2104294&clcid=0x409

The privacy statement can be viewed at:
https://go.microsoft.com/fwlink/?LinkId=853010&clcid=0x409

Configuring SQL Server...

Initial setup of Microsoft SQL Server failed. Please consult the ERRORLOG
in /var/opt/mssql/log for more information.

2)
recommand use Environment variables:
https://docs.microsoft.com/en-us/sql/linux/sql-server-linux-configure-environment-variables?view=sql-server-ver15
https://www.sqlshack.com/configure-sql-server-2017-linux-mssql-conf-available-tools/




##SQL BDC Cluster Deployment
SQL Server 2019 Big Data Clusters
https://www.youtube.com/watch?v=syFE77B6d14

[root@master tmp]# useradd stack
[root@master tmp]# echo redhat | passwd stack --stdin
Changing password for user stack.
passwd: all authentication tokens updated successfully.
[root@master tmp]# echo "stack ALL=(root) NOPASSWD:ALL" | tee -a /etc/sudoers.d/stack
stack ALL=(root) NOPASSWD:ALL
[root@master tmp]# chmod 0440 /etc/sudoers.d/stack
[root@master tmp]# su - stack
[stack@master ~]$ sudo ls
[stack@master ~]$ yum install wget
[stack@master ~]$ wget --no-check-certificate https://packages.microsoft.com/keys/microsoft.asc
[stack@master ~]$ sudo rpm --import microsoft.asc
[stack@master ~]$ sudo curl -k -o /etc/yum.repos.d/mssql-server.repo https://packages.microsoft.com/config/rhel/8/prod.repo
  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current
                                 Dload  Upload   Total   Spent    Left  Speed
100   192  100   192    0     0    240      0 --:--:-- --:--:-- --:--:--   240
[stack@master ~]$ sudo cat /etc/yum.repos.d/mssql-server.repo
[packages-microsoft-com-prod]
name=packages-microsoft-com-prod
baseurl=https://packages.microsoft.com/rhel/8/prod/
enabled=1
gpgcheck=1
gpgkey=https://packages.microsoft.com/keys/microsoft.asc
[stack@master ~]$
[stack@master ~]$ sudo yum -y install azdata-cli
[stack@master ~]$ azdata --version
20.3.1

Build (20210226.1)

SQL Server 2019 (15.0.4100)

Legal docs and information: https://aka.ms/eula-azdata-en

Python (Linux) 3.6.8 (default, Aug 24 2020, 17:57:11)
[GCC 8.3.1 20191121 (Red Hat 8.3.1-5)]

Python location '/usr/bin/python3'

[stack@master ~]$ azdata bdc config list
The privacy statement can be viewed at:
https://go.microsoft.com/fwlink/?LinkId=853010

The license terms for azdata can be viewed at:
https://aka.ms/eula-azdata-en

Do you accept the license terms? (y/n): y
[
  "aks-dev-test",
  "aks-dev-test-ha",
  "aro-dev-test",
  "aro-dev-test-ha",
  "kubeadm-dev-test",
  "kubeadm-prod",
  "openshift-dev-test",
  "openshift-prod"
]
[stack@master ~]$ azdata bdc config init --source kubeadm-dev-test --target custom
Option '--target' has been deprecated and will be removed in a future release. Use '--path' instead.
"Created configuration profile in custom"
[stack@master ~]$ vi custom/control.json
[stack@master ~]$ cat custom/control.json
{
    "apiVersion": "v1",
    "metadata": {
        "kind": "Cluster",
        "name": "mssql-cluster"
    },
    "spec": {
        "docker": {
            "registry": "mcr.microsoft.com",
            "repository": "mssql/bdc",
            "imageTag": "2019-CU9-ubuntu-16.04",
            "imagePullPolicy": "IfNotPresent"
        },
        "storage": {
            "data": {
                "className": "powerstore-xfs",
                "accessMode": "ReadWriteOnce",
                "size": "15Gi"
            },
            "logs": {
                "className": "powerstore-xfs",
                "accessMode": "ReadWriteOnce",
                "size": "10Gi"
            }
        },
        "endpoints": [
            {
                "name": "Controller",
                "serviceType": "NodePort",
                "port": 30080
            },
            {
                "name": "ServiceProxy",
                "serviceType": "NodePort",
                "port": 30777
            }
        ],
        "settings": {
            "controller": {
                "logs.rotation.size": "5000",
                "logs.rotation.days": "7"
            }
        }
    },
    "security": {}
}
[stack@master ~]$exit

[root@master tmp]# cp -a /home/stack/custom/ ./
[root@master tmp]# ls custom/
bdc.json  control.json
[root@master tmp]# vi custom/control.json
[root@master tmp]# azdata bdc create --config-profile custom --accept-eula yes
The privacy statement can be viewed at:
https://go.microsoft.com/fwlink/?LinkId=853010

The license terms for SQL Server Big Data Cluster can be viewed at:
Enterprise: https://go.microsoft.com/fwlink/?linkid=2104292
Standard: https://go.microsoft.com/fwlink/?linkid=2104294
Developer: https://go.microsoft.com/fwlink/?linkid=2104079


Cluster deployment documentation can be viewed at:
https://aka.ms/bdc-deploy

Azdata username:admin
Azdata password:
Confirm Azdata password:

NOTE: Cluster creation can take a significant amount of time depending on
configuration, network speed, and the number of nodes in the cluster.

Starting cluster deployment.
Waiting for cluster controller to start.
...

[root@master tmp]# kubectl get pods -n mssql-cluster
[root@master tmp]# kubectl get pvc -n mssql-cluster -L app,plane,role,type

#check BDC status
azdata login -n mssql-cluster
azdata bdc endpoint list -o table
azdata bdc status show
azdata bdc control status show
azdata bdc sql status show

azdata bdc endpoint list -e sql-server-master
Azure Data Studio (launch GUI console, connect to bdc "xxxip,31433)
https://docs.microsoft.com/en-us/sql/big-data-cluster/connect-to-big-data-cluster?view=sql-server-ver15

#BDC Architecture
https://www.youtube.com/watch?v=GvOo-VV7-p4
Control Plane
  -Controller(SQL Server Master,Knox Gateway,Livy,HIVE,SQL Cluster Administration Portal,Grafana Dashboard,Kibana Dashboard)
Compute Plane
  -Compute Pool(SQL Server,SQL Server...)
Data Plane
  -Data Pool(SQL Server)
  -Storage Pool(SQL Server,Spark,HDFS)
App Pool
  -App Pool(ML Server,Job(SSIS(SQL Server Integration Services)),Web Apps)
HDFS (Hadoop Distributed File System)

#Load sample data
https://github.com/microsoft/sql-server-samples/tree/master/samples/features/sql-big-data-cluster
curl -o bootstrap-sample-db.sh "https://raw.githubusercontent.com/Microsoft/sql-server-samples/master/samples/features/sql-big-data-cluster/bootstrap-sample-db.sh"
chmod +x bootstrap-sample-db.sh
curl -o bootstrap-sample-db.sql "https://raw.githubusercontent.com/Microsoft/sql-server-samples/master/samples/features/sql-big-data-cluster/bootstrap-sample-db.sql"
./bootstrap-sample-db.sh <CLUSTER_NAMESPACE> <SQL_MASTER_ENDPOINT> <KNOX_ENDPOINT>

#BDC workshop lab
https://microsoft.github.io/sqlworkshops/
https://github.com/microsoft/sqlworkshops-bdc
https://aka.ms/bdc-samples

#BDC offline deployments

#BDC upgrade
azdata bdc upgrade -n mssql-cluster -t 2019-CU5-ubuntu-16.04 -r mcr.microsoft.com/mssql/bdc


issue:
1)
[root@master ~]# kubectl get pods -w -n csi-powerstore
NAME                                     READY   STATUS             RESTARTS   AGE
powerstore-controller-7bc455d5c7-bn2q9   5/5     Running            184        16h
powerstore-controller-7bc455d5c7-d4fjd   4/5     CrashLoopBackOff   184        16h
powerstore-controller-7bc455d5c7-vfvgt   4/5     CrashLoopBackOff   184        16h
powerstore-node-6hljh                    2/2     Running            0          16h
powerstore-node-jbbds                    2/2     Running            0          16h
powerstore-node-xg7wp                    2/2     Running            0          16h
powerstore-controller-7bc455d5c7-bn2q9   4/5     Error              184        16h
powerstore-controller-7bc455d5c7-bn2q9   4/5     CrashLoopBackOff   184        16h


[root@master dell-csi-helm-installer]# kubectl logs -f powerstore-controller-7bc455d5c7-bn2q9 -c snapshotter

I0322 10:11:27.729241       1 leaderelection.go:248] failed to acquire lease csi-powerstore/external-snapshotter-leader-csi-powerstore-dellemc-com
E0322 10:12:06.262052       1 leaderelection.go:325] error retrieving resource lock csi-powerstore/external-snapshotter-leader-csi-powerstore-dellemc-com: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/csi-powerstore/leases/external-snapshotter-leader-csi-powerstore-dellemc-com": dial tcp 10.96.0.1:443: i/o timeout
I0322 10:12:06.262096       1 leaderelection.go:248] failed to acquire lease csi-powerstore/external-snapshotter-leader-csi-powerstore-dellemc-com
E0322 10:12:45.820648       1 leaderelection.go:325] error retrieving resource lock csi-powerstore/external-snapshotter-leader-csi-powerstore-dellemc-com: Get "https://10.96.0.1:443/apis/coordination.k8s.io/v1/namespaces/csi-powerstore/leases/external-snapshotter-leader-csi-powerstore-dellemc-com": dial tcp 10.96.0.1:443: i/o timeout
I0322 10:12:45.820688       1 leaderelection.go:248] failed to acquire lease csi-powerstore/external-snapshotter-leader-csi-powerstore-dellemc-com
^C
-->fix
#only use 2 node test, master and node1!!!
check /etc/hosts right lines!!!
192.168.1.20 master master
192.168.1.21 node1  node1
[root@master ~]# kubectl get pods
NAME                                     READY   STATUS    RESTARTS   AGE
powerstore-controller-7bc455d5c7-5zm85   5/5     Running   112        9h
powerstore-controller-7bc455d5c7-kqlrv   5/5     Running   10         9h
powerstore-node-rskw4                    2/2     Running   4          9h
powerstore-node-z7l8b                    2/2     Running   4          9h

[root@master ~]# kubectl logs powerstore-controller-7bc455d5c7-kqlrv snapshotter
I0613 02:56:56.060628       1 leaderelection.go:273] successfully renewed lease csi-powerstore/external-snapshotter-leader-csi-powerstore-dellemc-com
I0613 02:57:01.070784       1 leaderelection.go:273] successfully renewed lease csi-powerstore/external-snapshotter-leader-csi-powerstore-dellemc-com

[root@master ~]# kubectl logs powerstore-controller-7bc455d5c7-5zm85 snapshotter -f
I0613 02:58:49.876381       1 leaderelection.go:346] lock is held by powerstore-controller-7bc455d5c7-kqlrv and has not yet expired
I0613 02:58:49.876398       1 leaderelection.go:248] failed to acquire lease csi-powerstore/external-snapshotter-leader-csi-powerstore-dellemc-com

2)csi 1.4.0 driver install issue
[root@master dell-csi-helm-installer]# ./csi-install.sh --namespace csi-powerstore --values ./my-powerstore-settings.yaml
...
...
|- Verifying helm version                                           Success

------------------------------------------------------
> Verification Complete - Success
------------------------------------------------------
|
|- Installing Driver                                                Error: template: csi-powerstore/templates/controller.yaml:67:19: executing "csi-powerstore/templates/controller.yaml" at <.Values.replication.enabled>: nil pointer evaluating interface {}.enabled

------------------------------------------------------
Error: Helm operation failed, output can be found in /tmp/csi-install.127624.out. The failure should be examined, before proceeding. Additionally, running csi-uninstall.sh may be needed to clean up partial deployments.
Installation cannot continue
-->fix
#add "replication" to false!!!
[root@master dell-csi-helm-installer]# vim my-powerstore-settings.yaml
...
replication:
  enabled: false

3)
[root@master dell-csi-helm-installer]# kubectl logs -n csi-powerstore powerstore-node-8xhgz driver | grep -i "error\|failed"
{"level":"error","msg":"iscsi initiators found on node","time":"2021-06-22T16:28:08.568003871Z"}
{"level":"error","msg":"failed to read initiator ports names: FC is not supported for this host","time":"2021-06-22T16:28:08.568096312Z"}
{"level":"error","msg":"FC was not found or filtered with FCPortsFilterFile","time":"2021-06-22T16:28:08.568120251Z"}
{"level":"error","msg":"Get \"https://192.168.1.30/api/rest/host?limit=1000\u0026offset=0\u0026order=name\u0026select=id%2Cname%2Cdescription%2Chost_group_id%2Cimport_host_system_id%2Cos_type%2Ctype%2Chost_initiators\": tls: failed to parse certificate from server: asn1: structure error: integer not minimally-encoded","time":"2021-06-22T16:28:08.619686872Z"}
{"level":"error","msg":"can't setup host on https://192.168.1.30/api/rest: Get \"https://192.168.1.30/api/rest/host?limit=1000\u0026offset=0\u0026order=name\u0026select=id%2Cname%2Cdescription%2Chost_group_id%2Cimport_host_system_id%2Cos_type%2Ctype%2Chost_initiators\": tls: failed to parse certificate from server: asn1: structure error: integer not minimally-encoded","time":"2021-06-22T16:28:08.619779977Z"}
{"level":"error","msg":"Get \"https://192.168.1.30/api/rest/ip_pool_address?order=id\u0026purposes=cs.%7BStorage_Iscsi_Target%7D\u0026select=address%2Cappliance_id%2Cid%2Cip_port_id%2Cip_port%28target_iqn%2C+id%29%2Cnetwork_id%2Cnode_id%2Cpurposes\": tls: failed to parse certificate from server: asn1: structure error: integer not minimally-encoded","time":"2021-06-22T16:28:09.493292411Z"}
{"level":"error","msg":"couldn't get targets from array: Get \"https://192.168.1.30/api/rest/ip_pool_address?order=id\u0026purposes=cs.%7BStorage_Iscsi_Target%7D\u0026select=address%2Cappliance_id%2Cid%2Cip_port_id%2Cip_port%28target_iqn%2C+id%29%2Cnetwork_id%2Cnode_id%2Cpurposes\": tls: failed to parse certificate from server: asn1: structure error: integer not minimally-encoded","time":"2021-06-22T16:28:09.493310476Z"}
[root@master dell-csi-helm-installer]#
[root@master dell-csi-helm-installer]# kubectl logs -n csi-powerstore powerstore-controller-685ff6cb4b-g95w6 attacher | grep -i "error\|failed"
I0622 16:28:08.972809       1 connection.go:186] GRPC error: <nil>
I0622 16:28:08.973742       1 connection.go:186] GRPC error: <nil>
I0622 16:28:08.975031       1 connection.go:186] GRPC error: <nil>
I0622 16:28:08.977185       1 connection.go:186] GRPC error: <nil>
I0622 16:28:08.978389       1 connection.go:186] GRPC error: <nil>
I0622 16:28:09.014772       1 leaderelection.go:248] failed to acquire lease csi-powerstore/external-attacher-leader-csi-powerstore-dellemc-com
I0622 16:28:17.654198       1 leaderelection.go:248] failed to acquire lease csi-powerstore/external-attacher-leader-csi-powerstore-dellemc-com
[root@master dell-csi-helm-installer]# kubectl logs -n csi-powerstore powerstore-controller-685ff6cb4b-g95w6 provisioner | grep -i "error\|failed"
I0622 16:28:08.348313       1 connection.go:186] GRPC error: <nil>
I0622 16:28:08.349898       1 connection.go:186] GRPC error: <nil>
I0622 16:28:08.351509       1 connection.go:186] GRPC error: <nil>
I0622 16:28:08.352395       1 connection.go:186] GRPC error: <nil>
I0622 16:28:08.369140       1 leaderelection.go:248] failed to acquire lease csi-powerstore/csi-powerstore-dellemc-com
I0622 16:28:19.020188       1 leaderelection.go:248] failed to acquire lease csi-powerstore/csi-powerstore-dellemc-com

[root@master tmp]# kubectl get pvc
NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS     AGE
mssql-data    Pending                                      powerstore-xfs   9s
mssql-data2   Pending                                      powerstore-xfs   9s
mssql-log2    Pending                                      powerstore-xfs   9s
[root@master tmp]# kubectl describe pvc mssql-data
...
Events:
  Type     Reason                Age                From                                                                                                    Message
  ----     ------                ----               ----                                                                                                    -------
  Normal   Provisioning          14s (x5 over 27s)  csi-powerstore.dellemc.com_powerstore-controller-685ff6cb4b-rkk2q_72dc4dfb-a968-444c-9a81-de775162c97a  External provisioner is provisioning volume for claim "default/mssql-data"
  Warning  ProvisioningFailed    14s (x5 over 27s)  csi-powerstore.dellemc.com_powerstore-controller-685ff6cb4b-rkk2q_72dc4dfb-a968-444c-9a81-de775162c97a  failed to provision volume with StorageClass "powerstore-xfs": rpc error: code = Internal desc = Array IP's been provided, however it is not supported in current version. Configure you storage classes according to the documentation
  Normal   ExternalProvisioning  13s (x4 over 29s)  persistentvolume-controller                                                                             waiting for a volume to be created, either by external provisioner "csi-powerstore.dellemc.com" or manually created by system administrator


4)curl issue
cluster1 with firmware 1.0.0.0.4.038 have issue.
cluster2 with firmware 1.0.4.0.5.003 normal !!!
rhel8 and centos 8.4 same issue, will rhel7 curl is normal !!!

[root@master ~]#  curl -vvv -k --user admin:P@ssw0rd! https://192.168.1.30/api/rest/cluster?select=*
*   Trying 192.168.1.30...
* TCP_NODELAY set
* Connected to 192.168.1.30 (192.168.1.30) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (OUT), TLS alert, bad certificate (554):
* error:0D0E20DD:asn1 encoding routines:c2i_ibuf:illegal padding
* Closing connection 0
curl: (35) error:0D0E20DD:asn1 encoding routines:c2i_ibuf:illegal padding
[root@master ~]#

[root@master ~]#  curl -vvv -k --user admin:P@ssw0rd! https://192.168.1.40/api/rest/cluster?select=*
*   Trying 192.168.1.40...
* TCP_NODELAY set
* Connected to 192.168.1.40 (192.168.1.40) port 443 (#0)
* ALPN, offering h2
* ALPN, offering http/1.1
* successfully set certificate verify locations:
*   CAfile: /etc/pki/tls/certs/ca-bundle.crt
  CApath: none
* TLSv1.3 (OUT), TLS handshake, Client hello (1):
* TLSv1.3 (IN), TLS handshake, Server hello (2):
* TLSv1.2 (IN), TLS handshake, Certificate (11):
* TLSv1.2 (IN), TLS handshake, Server key exchange (12):
* TLSv1.2 (IN), TLS handshake, Request CERT (13):
* TLSv1.2 (IN), TLS handshake, Server finished (14):
* TLSv1.2 (OUT), TLS handshake, Certificate (11):
* TLSv1.2 (OUT), TLS handshake, Client key exchange (16):
* TLSv1.2 (OUT), TLS change cipher, Change cipher spec (1):
* TLSv1.2 (OUT), TLS handshake, Finished (20):
* TLSv1.2 (IN), TLS handshake, Finished (20):
* SSL connection using TLSv1.2 / ECDHE-RSA-AES256-GCM-SHA384
* ALPN, server did not agree to a protocol
* Server certificate:
*  subject: C=US + O=Dell + L=Hopkinton + OU=PowerStore + ST=Massachusetts + CN=ManagementHTTP.PSec370360bc60
*  start date: Jun 21 03:11:23 2021 GMT
*  expire date: Jun 20 03:11:23 2026 GMT
*  issuer: C=US; ST=MA; O=Dell EMC; CN=Dell EMC PowerStore CA YZ4USM9E
*  SSL certificate verify result: self signed certificate in certificate chain (19), continuing anyway.
* Server auth using Basic with user 'admin'
> GET /api/rest/cluster?select=* HTTP/1.1
> Host: 192.168.1.40
> Authorization: Basic YWRtaW46UEBzc3cwcmQh
> User-Agent: curl/7.61.1
> Accept: */*
>
< HTTP/1.1 200 OK
< DELL-EMC-TOKEN: IhJC/04NmCqSmLQxvgyVSsnebB8A/0qHDa+K3LkhXJY=
< Content-Type: application/json
< transfer-encoding: chunked
< content-language: en-US
< set-cookie: auth_cookie=dae0401ca49609c01f6102c5f9fa5191; Path=/; Secure; HTTPOnly
<
* Connection #0 to host 192.168.1.40 left intact
[{"id":"0","global_id":"PSec370360bc60","name":"POD3-Cluster2","physical_mtu":1500,"master_appliance_id":"A1","state":"Configured","appliance_count":1,"management_address":"192.168.1.40","is_encryption_enabled":true,"storage_discovery_address":"192.168.2.40","compatibility_level":1,"state_l10n":"Configured"}]
[root@master ~]#


[root@linux17 ~]# curl -vvv -k --user admin:P@ssw0rd! https://192.168.1.30/api/rest/cluster?select=*
* About to connect() to 192.168.1.30 port 443 (#0)
*   Trying 192.168.1.30...
* Connected to 192.168.1.30 (192.168.1.30) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* skipping SSL peer certificate verification
* NSS: client certificate not found (nickname not specified)
* SSL connection using TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384
* Server certificate:
*       subject: C=US+O=Dell+L=Hopkinton+OU=PowerStore+ST=Massachusetts+CN=ManagementHTTP.PS04e694d0c442
*       start date: Jun 21 03:09:45 2021 GMT
*       expire date: Jun 20 03:09:45 2026 GMT
*       common name: ManagementHTTP.PS04e694d0c442
*       issuer: CN=Dell EMC PowerStore CA ZTUGYPNK,O=Dell EMC,ST=MA,C=US
* Server auth using Basic with user 'admin'
> GET /api/rest/cluster?select=* HTTP/1.1
> Authorization: Basic YWRtaW46UEBzc3cwcmQh
> User-Agent: curl/7.29.0
> Host: 192.168.1.30
> Accept: */*
>
< HTTP/1.1 200 OK
< DELL-EMC-TOKEN: IrfmhMXOORUkBZ0RGzAPV1AK+O07s5uRbULnslrRr8s=
< Content-Type: application/json
< transfer-encoding: chunked
< content-language: en-US
< set-cookie: auth_cookie=b5557d07ba1347883106689382b97a74; Path=/; Secure; HTTPOnly
<
* Connection #0 to host 192.168.1.30 left intact
[{"id":"0","global_id":"PS04e694d0c442","name":"POD3-Cluster1","physical_mtu":1500,"master_appliance_id":"A1","state":"Configured","appliance_count":1,"management_address":"192.168.1.30","is_encryption_enabled":true,"storage_discovery_address":"192.168.2.30","compatibility_level":1,"state_l10n":"Configured"}]
[root@linux17 ~]#
